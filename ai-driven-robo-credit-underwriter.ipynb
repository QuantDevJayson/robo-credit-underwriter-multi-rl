{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimized AI Robo-Credit Underwriter with Multi-Agent RL & Risk-Aware Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Introduction\n",
    "Traditional credit underwriting relies on rule-based scoring models and statistical risk assessments, which often fail to adapt to dynamic financial environments. With the advancement of Artificial Intelligence (AI), particularly Reinforcement Learning (RL), credit decision-making can be enhanced by autonomous agents that optimize risk-return trade-offs in real-time. This project develops an AI-driven Robo-Credit Underwriter, integrating Multi-Agent Reinforcement Learning (PPO & DQN) to balance credit approvals and risk management. Additionally, Risk-Aware Policy Learning (Conditional Value at Risk - CVaR & Bayesian estimation) ensures responsible lending. A FastAPI backend serves real-time predictions, while a Streamlit UI enables interactive loan applications. This approach modernizes credit risk evaluation by leveraging AI for optimal, explainable, and adaptive decision-making.\n",
    "\n",
    "#### Literature Review\n",
    "AI and machine learning have transformed credit risk assessment, with deep learning and reinforcement learning emerging as key methodologies. Traditional models, such as logistic regression and decision trees, struggle with non-linear credit risk patterns (Lessmann et al., 2015). Deep learning improves default prediction accuracy but lacks interpretability (Leong et al., 2022). Reinforcement learning has been explored for financial decision-making (Dixon et al., 2020), with Deep Q-Learning (Mnih et al., 2015) and PPO (Schulman et al., 2017) demonstrating robust adaptability. Multi-Agent RL (Li et al., 2021) further enhances risk-aware credit scoring. SHAP (Lundberg & Lee, 2017) provides interpretability, bridging AI and regulatory transparency. Risk-aware policy learning via CVaR (Rockafellar & Uryasev, 2000) ensures downside risk mitigation, making AI credit underwriting safer and more robust."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from fastapi import FastAPI\n",
    "from gymnasium import spaces\n",
    "from stable_baselines3 import PPO, DQN\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from pydantic import BaseModel\n",
    "from risk_policy import risk_adjusted_approval\n",
    "import streamlit as st\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I. Generate Synthetic Credit Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synthetic dataset generated: synthetic_credit_data.csv\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "N = 5000  # Number of samples\n",
    "\n",
    "data = {\n",
    "    \"credit_score\": np.random.randint(300, 850, N),\n",
    "    \"income\": np.random.randint(20000, 150000, N),\n",
    "    \"debt_to_income\": np.random.uniform(0.1, 0.9, N),\n",
    "    \"age\": np.random.randint(21, 70, N),\n",
    "    \"employment_years\": np.random.randint(0, 40, N),\n",
    "    \"loan_amount\": np.random.randint(5000, 50000, N),\n",
    "    \"interest_rate\": np.random.uniform(1, 15, N),\n",
    "    \"approved\": np.random.choice([0, 1], N, p=[0.3, 0.7]),\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df[\"default_risk\"] = np.where(df[\"credit_score\"] < 600, np.random.uniform(0.4, 0.9, N), np.random.uniform(0.01, 0.3, N))\n",
    "df.to_csv(\"synthetic_credit_data.csv\", index=False)\n",
    "print(\"Synthetic dataset generated: synthetic_credit_data.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- #### Train Credit Approval Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "credit_model.pkl saved successfully!\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "df = pd.read_csv(\"synthetic_credit_data.csv\")  \n",
    "\n",
    "# Features & Target\n",
    "X = df.drop(columns=[\"approved\"])  \n",
    "y = df[\"approved\"]  \n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Feature scaling\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Train Random Forest Model\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Save the model\n",
    "with open(\"credit_model.pkl\", \"wb\") as f:\n",
    "    pickle.dump(model, f)\n",
    "\n",
    "print(\"credit_model.pkl saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### II. Train Multi-Agent RL (PPO & DQN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "@app.get(\"/\")\n",
    "def read_root():\n",
    "    return {\"message\": \"API is running!\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- #### Reinforcement Learning Environment\n",
    "This custom Gym environment trains RL agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CreditEnv initialized!\n"
     ]
    }
   ],
   "source": [
    "# Load synthetic data\n",
    "df = pd.read_csv(\"synthetic_credit_data.csv\")\n",
    "\n",
    "class CreditEnv(gym.Env):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.observation_space = spaces.Box(low=0, high=1, shape=(8,), dtype=np.float32)\n",
    "        self.action_space = spaces.Discrete(2)  # Approve (1) / Reject (0)\n",
    "        self.data = df.sample(frac=1).reset_index(drop=True)\n",
    "        self.index = 0\n",
    "\n",
    "        # Define state & action space\n",
    "        self.observation_space = spaces.Box(low=0, high=1, shape=(8,), dtype=np.float32)\n",
    "        self.action_space = spaces.Discrete(2)  # Approve (1) or Reject (0)\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        self.index = 0\n",
    "        obs = self.data.iloc[self.index, :-2].values.astype(np.float32)\n",
    "        \n",
    "        #  Ensure correct shape (8,)\n",
    "        if obs.shape[0] != 8:\n",
    "            obs = np.pad(obs, (0, 8 - obs.shape[0]), mode='constant')\n",
    "\n",
    "        return obs, {}\n",
    "\n",
    "    def step(self, action):\n",
    "        row = self.data.iloc[self.index]\n",
    "        reward = (1 if action == row[\"approved\"] else -1) - (3 if row[\"default_risk\"] > 0.5 else 0)\n",
    "        self.index += 1\n",
    "        done = self.index >= len(self.data)\n",
    "\n",
    "        if not done:\n",
    "            obs = self.data.iloc[self.index, :-2].values.astype(np.float32)\n",
    "            # Ensure correct shape (8,)\n",
    "            if obs.shape[0] != 8:\n",
    "                obs = np.pad(obs, (0, 8 - obs.shape[0]), mode='constant')\n",
    "        else:\n",
    "            obs = np.zeros(8)\n",
    "\n",
    "        return obs, reward, done, False, {}\n",
    "\n",
    "# Test the environment\n",
    "if __name__ == \"__main__\":\n",
    "    env = CreditEnv()\n",
    "    print(\"CreditEnv initialized!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- #### Train RL Agents \n",
    "This script trains PPO & DQN RL agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 625  |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 3    |\n",
      "|    total_timesteps | 2048 |\n",
      "-----------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 438        |\n",
      "|    iterations           | 2          |\n",
      "|    time_elapsed         | 9          |\n",
      "|    total_timesteps      | 4096       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01858196 |\n",
      "|    clip_fraction        | 0.187      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.676     |\n",
      "|    explained_variance   | -0.00324   |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 45.9       |\n",
      "|    n_updates            | 10         |\n",
      "|    policy_gradient_loss | -0.016     |\n",
      "|    value_loss           | 205        |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 5e+03       |\n",
      "|    ep_rew_mean          | -6e+03      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 387         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014243913 |\n",
      "|    clip_fraction        | 0.141       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.635      |\n",
      "|    explained_variance   | 0.00125     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 89.6        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.012      |\n",
      "|    value_loss           | 251         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 5e+03       |\n",
      "|    ep_rew_mean          | -6e+03      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 365         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 22          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008677945 |\n",
      "|    clip_fraction        | 0.0787      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.586      |\n",
      "|    explained_variance   | -0.000653   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 62.7        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.00698    |\n",
      "|    value_loss           | 183         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 5e+03       |\n",
      "|    ep_rew_mean          | -5.67e+03   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 343         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 29          |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008053865 |\n",
      "|    clip_fraction        | 0.0644      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.53       |\n",
      "|    explained_variance   | -0.000105   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 38.5        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.00496    |\n",
      "|    value_loss           | 129         |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 5e+03        |\n",
      "|    ep_rew_mean          | -5.67e+03    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 322          |\n",
      "|    iterations           | 6            |\n",
      "|    time_elapsed         | 38           |\n",
      "|    total_timesteps      | 12288        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0051506236 |\n",
      "|    clip_fraction        | 0.0602       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.472       |\n",
      "|    explained_variance   | -1.44e-05    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 27.8         |\n",
      "|    n_updates            | 50           |\n",
      "|    policy_gradient_loss | -0.00487     |\n",
      "|    value_loss           | 104          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 5e+03        |\n",
      "|    ep_rew_mean          | -5.67e+03    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 306          |\n",
      "|    iterations           | 7            |\n",
      "|    time_elapsed         | 46           |\n",
      "|    total_timesteps      | 14336        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0043969015 |\n",
      "|    clip_fraction        | 0.0601       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.411       |\n",
      "|    explained_variance   | -9.66e-05    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 17.3         |\n",
      "|    n_updates            | 60           |\n",
      "|    policy_gradient_loss | -0.0047      |\n",
      "|    value_loss           | 70.1         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 5e+03        |\n",
      "|    ep_rew_mean          | -5.39e+03    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 294          |\n",
      "|    iterations           | 8            |\n",
      "|    time_elapsed         | 55           |\n",
      "|    total_timesteps      | 16384        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0038949614 |\n",
      "|    clip_fraction        | 0.0624       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.356       |\n",
      "|    explained_variance   | 9.19e-05     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 14.3         |\n",
      "|    n_updates            | 70           |\n",
      "|    policy_gradient_loss | -0.00572     |\n",
      "|    value_loss           | 62           |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 5e+03       |\n",
      "|    ep_rew_mean          | -5.39e+03   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 297         |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 62          |\n",
      "|    total_timesteps      | 18432       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003630075 |\n",
      "|    clip_fraction        | 0.0374      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.314      |\n",
      "|    explained_variance   | 2.72e-05    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 13.7        |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.00326    |\n",
      "|    value_loss           | 57.9        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 5e+03        |\n",
      "|    ep_rew_mean          | -5.23e+03    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 299          |\n",
      "|    iterations           | 10           |\n",
      "|    time_elapsed         | 68           |\n",
      "|    total_timesteps      | 20480        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0060809427 |\n",
      "|    clip_fraction        | 0.0385       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.275       |\n",
      "|    explained_variance   | 1.34e-05     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 13.9         |\n",
      "|    n_updates            | 90           |\n",
      "|    policy_gradient_loss | -0.00177     |\n",
      "|    value_loss           | 37.6         |\n",
      "------------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 5e+03      |\n",
      "|    ep_rew_mean          | -5.23e+03  |\n",
      "| time/                   |            |\n",
      "|    fps                  | 302        |\n",
      "|    iterations           | 11         |\n",
      "|    time_elapsed         | 74         |\n",
      "|    total_timesteps      | 22528      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00265063 |\n",
      "|    clip_fraction        | 0.043      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.248     |\n",
      "|    explained_variance   | 1.43e-06   |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 13.4       |\n",
      "|    n_updates            | 100        |\n",
      "|    policy_gradient_loss | -0.00129   |\n",
      "|    value_loss           | 48.8       |\n",
      "----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 5e+03        |\n",
      "|    ep_rew_mean          | -5.23e+03    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 304          |\n",
      "|    iterations           | 12           |\n",
      "|    time_elapsed         | 80           |\n",
      "|    total_timesteps      | 24576        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0022119998 |\n",
      "|    clip_fraction        | 0.032        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.219       |\n",
      "|    explained_variance   | -7.27e-06    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 9.92         |\n",
      "|    n_updates            | 110          |\n",
      "|    policy_gradient_loss | -0.00209     |\n",
      "|    value_loss           | 25.4         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 5e+03       |\n",
      "|    ep_rew_mean          | -5.08e+03   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 306         |\n",
      "|    iterations           | 13          |\n",
      "|    time_elapsed         | 86          |\n",
      "|    total_timesteps      | 26624       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002435462 |\n",
      "|    clip_fraction        | 0.0217      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.194      |\n",
      "|    explained_variance   | 3.79e-05    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 9.56        |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.00214    |\n",
      "|    value_loss           | 26          |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 5e+03        |\n",
      "|    ep_rew_mean          | -5.08e+03    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 307          |\n",
      "|    iterations           | 14           |\n",
      "|    time_elapsed         | 93           |\n",
      "|    total_timesteps      | 28672        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0019601693 |\n",
      "|    clip_fraction        | 0.0141       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.159       |\n",
      "|    explained_variance   | 2.5e-06      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 13.7         |\n",
      "|    n_updates            | 130          |\n",
      "|    policy_gradient_loss | -0.0011      |\n",
      "|    value_loss           | 49.1         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 5e+03        |\n",
      "|    ep_rew_mean          | -4.96e+03    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 307          |\n",
      "|    iterations           | 15           |\n",
      "|    time_elapsed         | 99           |\n",
      "|    total_timesteps      | 30720        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0013919964 |\n",
      "|    clip_fraction        | 0.0143       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.138       |\n",
      "|    explained_variance   | 1.09e-05     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 12.7         |\n",
      "|    n_updates            | 140          |\n",
      "|    policy_gradient_loss | -0.000475    |\n",
      "|    value_loss           | 25.3         |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 5e+03         |\n",
      "|    ep_rew_mean          | -4.96e+03     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 308           |\n",
      "|    iterations           | 16            |\n",
      "|    time_elapsed         | 106           |\n",
      "|    total_timesteps      | 32768         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00047221832 |\n",
      "|    clip_fraction        | 0.0139        |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.102        |\n",
      "|    explained_variance   | 1.69e-05      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 15.2          |\n",
      "|    n_updates            | 150           |\n",
      "|    policy_gradient_loss | -0.000905     |\n",
      "|    value_loss           | 47.9          |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 5e+03         |\n",
      "|    ep_rew_mean          | -4.96e+03     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 309           |\n",
      "|    iterations           | 17            |\n",
      "|    time_elapsed         | 112           |\n",
      "|    total_timesteps      | 34816         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00031086282 |\n",
      "|    clip_fraction        | 0.00693       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.102        |\n",
      "|    explained_variance   | -8.33e-05     |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 11.8          |\n",
      "|    n_updates            | 160           |\n",
      "|    policy_gradient_loss | -4.07e-05     |\n",
      "|    value_loss           | 24.2          |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 5e+03         |\n",
      "|    ep_rew_mean          | -4.87e+03     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 310           |\n",
      "|    iterations           | 18            |\n",
      "|    time_elapsed         | 118           |\n",
      "|    total_timesteps      | 36864         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00060223625 |\n",
      "|    clip_fraction        | 0.00723       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.0911       |\n",
      "|    explained_variance   | 7.13e-05      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 13.1          |\n",
      "|    n_updates            | 170           |\n",
      "|    policy_gradient_loss | -0.00133      |\n",
      "|    value_loss           | 23.6          |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 5e+03        |\n",
      "|    ep_rew_mean          | -4.87e+03    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 310          |\n",
      "|    iterations           | 19           |\n",
      "|    time_elapsed         | 125          |\n",
      "|    total_timesteps      | 38912        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0003025686 |\n",
      "|    clip_fraction        | 0.00459      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0821      |\n",
      "|    explained_variance   | -0.000198    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 28           |\n",
      "|    n_updates            | 180          |\n",
      "|    policy_gradient_loss | 0.000294     |\n",
      "|    value_loss           | 50.6         |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 5e+03         |\n",
      "|    ep_rew_mean          | -4.8e+03      |\n",
      "| time/                   |               |\n",
      "|    fps                  | 309           |\n",
      "|    iterations           | 20            |\n",
      "|    time_elapsed         | 132           |\n",
      "|    total_timesteps      | 40960         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00075070176 |\n",
      "|    clip_fraction        | 0.00781       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.0615       |\n",
      "|    explained_variance   | -1.97e-05     |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 12.3          |\n",
      "|    n_updates            | 190           |\n",
      "|    policy_gradient_loss | -0.00109      |\n",
      "|    value_loss           | 24.7          |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 5e+03         |\n",
      "|    ep_rew_mean          | -4.8e+03      |\n",
      "| time/                   |               |\n",
      "|    fps                  | 309           |\n",
      "|    iterations           | 21            |\n",
      "|    time_elapsed         | 139           |\n",
      "|    total_timesteps      | 43008         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00016951293 |\n",
      "|    clip_fraction        | 0.000977      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.0517       |\n",
      "|    explained_variance   | 7.03e-05      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 22.9          |\n",
      "|    n_updates            | 200           |\n",
      "|    policy_gradient_loss | 4.1e-06       |\n",
      "|    value_loss           | 50.1          |\n",
      "-------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 5e+03       |\n",
      "|    ep_rew_mean          | -4.74e+03   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 308         |\n",
      "|    iterations           | 22          |\n",
      "|    time_elapsed         | 145         |\n",
      "|    total_timesteps      | 45056       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.000514465 |\n",
      "|    clip_fraction        | 0.00234     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.0478     |\n",
      "|    explained_variance   | -0.00143    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 12.4        |\n",
      "|    n_updates            | 210         |\n",
      "|    policy_gradient_loss | 9.88e-06    |\n",
      "|    value_loss           | 23.5        |\n",
      "-----------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 5e+03         |\n",
      "|    ep_rew_mean          | -4.74e+03     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 307           |\n",
      "|    iterations           | 23            |\n",
      "|    time_elapsed         | 153           |\n",
      "|    total_timesteps      | 47104         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00020285277 |\n",
      "|    clip_fraction        | 0.00396       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.0318       |\n",
      "|    explained_variance   | 0.000127      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 14.1          |\n",
      "|    n_updates            | 220           |\n",
      "|    policy_gradient_loss | -0.000483     |\n",
      "|    value_loss           | 54.5          |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 5e+03         |\n",
      "|    ep_rew_mean          | -4.74e+03     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 307           |\n",
      "|    iterations           | 24            |\n",
      "|    time_elapsed         | 159           |\n",
      "|    total_timesteps      | 49152         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00036235398 |\n",
      "|    clip_fraction        | 0.00449       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.0241       |\n",
      "|    explained_variance   | -0.00047      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 12.1          |\n",
      "|    n_updates            | 230           |\n",
      "|    policy_gradient_loss | -0.000433     |\n",
      "|    value_loss           | 22.8          |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 5e+03         |\n",
      "|    ep_rew_mean          | -4.69e+03     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 305           |\n",
      "|    iterations           | 25            |\n",
      "|    time_elapsed         | 167           |\n",
      "|    total_timesteps      | 51200         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00025257314 |\n",
      "|    clip_fraction        | 0.00278       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.0278       |\n",
      "|    explained_variance   | 7.23e-05      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 12.5          |\n",
      "|    n_updates            | 240           |\n",
      "|    policy_gradient_loss | -0.000257     |\n",
      "|    value_loss           | 23            |\n",
      "-------------------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 5e+03     |\n",
      "|    ep_rew_mean      | -6.12e+03 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 4         |\n",
      "|    fps              | 368       |\n",
      "|    time_elapsed     | 54        |\n",
      "|    total_timesteps  | 20000     |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0001    |\n",
      "|    loss             | 3.09e+03  |\n",
      "|    n_updates        | 4974      |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 5e+03     |\n",
      "|    ep_rew_mean      | -6.09e+03 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 8         |\n",
      "|    fps              | 374       |\n",
      "|    time_elapsed     | 106       |\n",
      "|    total_timesteps  | 40000     |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0001    |\n",
      "|    loss             | 1.82e+03  |\n",
      "|    n_updates        | 9974      |\n",
      "-----------------------------------\n",
      "Multi-Agent RL Training Complete!\n"
     ]
    }
   ],
   "source": [
    "# Initialize environment\n",
    "env = CreditEnv()\n",
    "\n",
    "# Use PPO and DQN (since DDPG requires continuous actions)\n",
    "ppo_agent = PPO(\"MlpPolicy\", env, verbose=1)\n",
    "dqn_agent = DQN(\"MlpPolicy\", env, verbose=1)\n",
    "\n",
    "# Train the agents\n",
    "ppo_agent.learn(total_timesteps=50000)\n",
    "dqn_agent.learn(total_timesteps=50000)\n",
    "\n",
    "# Save models\n",
    "ppo_agent.save(\"ppo_credit_agent\")\n",
    "dqn_agent.save(\"dqn_risk_control_agent\")\n",
    "\n",
    "print(\"Multi-Agent RL Training Complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### III. Risk-Aware Policy Learning (CVaR & Bayesian Estimation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cvar(returns, alpha=0.05):\n",
    "    return np.mean(np.sort(returns)[:int(len(returns) * alpha)])\n",
    "\n",
    "def risk_adjusted_approval(default_prob, loan_amount, interest_rate):\n",
    "    return int((1 - default_prob) * loan_amount * (1 + interest_rate) - default_prob * loan_amount > 5000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IV. FastAPI Backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !run: uvicorn api:app --reload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### V. Streamlit UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !run:  streamlit run app.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Further Advancement:\n",
    "\n",
    "- Extend to include real-time financial risk indicators (e.g., market trends), make the model robust, yet simple and not over-complicated for practical application!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "**Dixon, M., Halperin, I., & Bilokon, P. (2020).** Machine learning in finance: From theory to practice. Springer. <br>\n",
    "**Leong, C., Tan, B., Xiao, X., & Tan, F. (2022).** Explainable AI in credit scoring: Challenges and opportunities. Journal of Financial Innovation, 8(1), 22–37.<br>\n",
    "**Lessmann, S., Baesens, B., Seow, H. V., & Thomas, L. C. (2015)**. Benchmarking state-of-the-art classification algorithms for credit scoring. Journal of the Operational Research Society, 66(1), 1–23.<br>\n",
    "**Li, T., Yang, Y., & Wang, J. (2021).** Multi-agent reinforcement learning in finance: Applications and challenges. Finance and AI Review, 10(3), 55–78.<br>\n",
    "**Lundberg, S. M., & Lee, S. I. (2017).** A unified approach to interpreting model predictions. Advances in Neural Information Processing Systems (NeurIPS), 30.<br>\n",
    "**Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J., Bellemare, M. G., ... & Hassabis, D. (2015).** Human-level control through deep reinforcement learning. Nature, 518(7540), 529-533.<br>\n",
    "**Rockafellar, R. T., & Uryasev, S. (2000).** Optimization of conditional value-at-risk. Journal of Risk, 2(3), 21–42.<br>\n",
    "**Schulman, J., Wolski, F., Dhariwal, P., Radford, A., & Klimov, O. (2017)**. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347.<br>\n",
    "**Van Gestel, T., & Baesens, B. (2009)**. Credit risk management: Basic concepts: Financial risk components, rating analysis, models, economic and regulatory capital. Oxford University Press.<br>\n",
    "**Zhou, Y., & Wang, Z. (2021)**. Deep reinforcement learning for credit risk assessment: A comparative study. Expert Systems with Applications, 174, 114670.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
